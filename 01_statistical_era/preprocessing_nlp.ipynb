{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c591487f",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "<p> Before 2010, if you wanted to build a spam filter or a chatbot, you didn't use a \"Brain.\" You used a Calculator.</p>\n",
    "<p>Must of time we spent on Data cleaning part.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81184e",
   "metadata": {},
   "source": [
    "## 1. The Preprocessing Flow\n",
    "\n",
    "<p>Input: \"The 2 dogs were running fast!!\"</p>\n",
    "\n",
    "| Step | Action | Result (Current State) |\n",
    "|:-----------|:------------:|------------:|\n",
    "| 1. Lowercasing       | Normalize case            | \"the 2 dogs were running fast!!\"|\n",
    "| 2. Noise Removal     | Remove symbols/nums       | \"the dogs were running fast\"           |\n",
    "| 3. Tokenization      | Split into list           | [\"the\", \"dogs\", \"were\", \"running\", \"fast\"]          |\n",
    "| 4. Stop Words        | Remove \"the\", \"were\"      | [\"dogs\", \"running\", \"fast\"]          |\n",
    "| 5. Stemming          | Chop suffixes             | [\"dog\", \"run\", \"fast\"]          |\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "1. **Stemming**\n",
    "\n",
    "- Stemming is a Rule-Based approach. \n",
    "- It doesn't know English; it just knows patterns. It uses a list of if/else rules to strip suffixes.\n",
    "\n",
    "**Rules**\n",
    "- Rule: If a word ends in \"ing\", remove \"ing\".\n",
    "- Rule: If a word ends in \"ed\", remove \"ed\".\n",
    "\n",
    "Examples of Failure (Over-chopping)\n",
    "- Input: \"Universities\"\n",
    "- Stemmer: \"Universit\" (Not a real word).\n",
    "- Input: \"Ponies\"\n",
    "- Stemmer: \"Poni\" (Not a real word).\n",
    "\n",
    "**Why do we use it?**\n",
    "\n",
    "It is incredibly fast and efficient for search engines. If you search for \"Fishing,\" a stemmer instantly matches it to documents containing \"Fish.\" The fact that \"Fish\" isn't exactly \"Fishing\" doesn't matter for search.\n",
    "\n",
    "\n",
    "2. **Lemmatization**\n",
    "\n",
    "- Lemmatization is a Linguistic approach. It performs a \"Morphological Analysis.\"\n",
    "- It looks at the word, determines its Part of Speech (Noun, Verb, Adjective), and then looks up the root form (Lemma) in a database (like WordNet).\n",
    "- Lemma - The \"Base Form\" of a word. Example:\n",
    "\n",
    "| Word Variation (Inflection) | The Lemma | \n",
    "|:-----------|------------:|\n",
    "| 1. Running, Ran, Runs       | Run            | \n",
    "| 2. Better       | Good            | \n",
    "| 3. Mice       | Mouse            | \n",
    "| 4. Corpora           | Corpus            | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639ad67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'dog', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    \"\"\"\n",
    "    Input: \"The Dog run!!\"\n",
    "    Output: ['dog', 'run']\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # 3. Tokenize (Split by space)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "input = \"The Dog run!!\"\n",
    "print(simple_preprocess(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946cda01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original        | Stemming             | Lemmatization       \n",
      "-----------------------------------------------------------------\n",
      "running         | run                  | run                 \n",
      "flies           | fli                  | fly                 \n",
      "universities    | univers              | universities        \n",
      "better          | better               | better              \n",
      "ate             | ate                  | eat                 \n"
     ]
    }
   ],
   "source": [
    "# 05_preprocessing_deep_dive.ipynb\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the dictionary (Run this once)\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the tools\n",
    "stemmer = PorterStemmer()      # Stammening tool\n",
    "lemmatizer = WordNetLemmatizer() # lamentization tool\n",
    "\n",
    "# --- Test Data ---\n",
    "words = [\"running\", \"flies\", \"universities\", \"better\", \"ate\"]\n",
    "\n",
    "print(f\"{'Original':<15} | {'Stemming ':<20} | {'Lemmatization':<20}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for word in words:\n",
    "    # 1. Stemming\n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    # 2. Lemmatization (We assume everything is a Verb 'v' or Noun 'n' for demo)\n",
    "    # Note: 'ate' requires knowing it is a verb to become 'eat'\n",
    "    lemma = lemmatizer.lemmatize(word, pos='v') \n",
    "    \n",
    "    print(f\"{word:<15} | {stem:<20} | {lemma:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
